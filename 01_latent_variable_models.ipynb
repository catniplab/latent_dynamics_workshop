{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d87286532e00cc67",
   "metadata": {
    "id": "d87286532e00cc67"
   },
   "source": [
    "# Latent Variable Models: PCA to Kalman Filtering\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/catniplab/latent_dynamics_workshop/blob/main/01_latent_variable_models.ipynb)\n",
    "\n",
    "In this notebook, we'll simulate a 2D latent dynamical system that generates high-dimensional observations, and then explore how different latent variable models (PCA, Factor Analysis, and Kalman Filtering) can be used to infer the hidden states.\n",
    "\n",
    "We'll see how progressively richer statistical models help us recover latent structure more accurately by incorporating better probabilistic modeling and temporal dynamics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c246630a-5e5d-47a1-ae3d-f6d1da94139a",
   "metadata": {
    "id": "c246630a-5e5d-47a1-ae3d-f6d1da94139a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    _in_colab = True\n",
    "except:\n",
    "    _in_colab = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d92a9ef-a475-4dbc-9d3f-3c5315185d5f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7d92a9ef-a475-4dbc-9d3f-3c5315185d5f",
    "outputId": "2effd3a6-e465-40fd-ca8c-0fee1f2e5119",
    "tags": []
   },
   "outputs": [],
   "source": [
    "if _in_colab:\n",
    "    !git clone --recurse-submodules https://github.com/catniplab/latent_dynamics_workshop.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26c46d5-5329-4579-908d-b0ca7d80b2d2",
   "metadata": {
    "id": "a26c46d5-5329-4579-908d-b0ca7d80b2d2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "cwd = os.getcwd()\n",
    "if _in_colab:\n",
    "    sys.path.append(os.path.join(cwd, \"latent_dynamics_workshop\"))\n",
    "    sys.path.append(os.path.join(cwd, \"latent_dynamics_workshop/xfads\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5kwHsROHVszn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5kwHsROHVszn",
    "outputId": "af25fff6-0ea7-456d-c280-ec273467a46c"
   },
   "outputs": [],
   "source": [
    "if _in_colab:\n",
    "    !pip install -e latent_dynamics_workshop/xfads/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce41247adf239f75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T19:37:31.716357Z",
     "start_time": "2025-07-13T19:37:28.670043Z"
    },
    "id": "ce41247adf239f75",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import xfads.utils as utils\n",
    "import xfads.plot_utils as plot_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import xfads.prob_utils as prob_utils\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from hydra import compose, initialize\n",
    "from sklearn.decomposition import PCA\n",
    "from xfads.linalg_utils import bmv, chol_bmv_solve, triangular_inverse\n",
    "from xfads.prob_utils import kalman_information_filter, rts_smoother, align_latent_variables, construct_hankel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5944ab8104180f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T19:37:33.288050Z",
     "start_time": "2025-07-13T19:37:33.127124Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ab5944ab8104180f",
    "outputId": "5c7701b0-6705-49b0-a73d-499f8a088bc2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"config\"\"\"\n",
    "\n",
    "cfg_dict = {\n",
    "    'n_latents': 2,\n",
    "    'device': 'cuda',\n",
    "    'default_dtype': torch.float32,\n",
    "    'seed': 1234,\n",
    "}\n",
    "\n",
    "class Cfg(dict):\n",
    "    def __getattr__(self, attr):\n",
    "        if attr in self:\n",
    "            return self[attr]\n",
    "        else:\n",
    "            raise AttributeError(f\"'Cfg' object has no attribute '{attr}'\")\n",
    "\n",
    "cfg = Cfg(cfg_dict)\n",
    "\n",
    "# Set devices and seed\n",
    "if not torch.cuda.is_available():\n",
    "    cfg.device = 'cpu'\n",
    "\n",
    "pl.seed_everything(cfg.seed, workers=True)\n",
    "torch.set_default_dtype(cfg.default_dtype)\n",
    "\n",
    "if cfg.device == 'cuda':\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bedb7c7f3845cd1",
   "metadata": {
    "id": "6bedb7c7f3845cd1"
   },
   "source": [
    "## Simulating Data from a Latent Dynamical System\n",
    "First, let's simulate data from a linear dynamical system; here, observations represent real valued 'neural activity' read out from a lower dimensional latent state.  This corresponds to a generative model formulated as,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{z}_1 &\\sim \\mathcal{N}(0, \\mathbf{Q}_0) \\\\\\\\\n",
    "\\mathbf{z}_t &= \\mathbf{A} \\mathbf{z}_{t-1} + \\mathbf{w}_t \\\\\\\\\n",
    "\\mathbf{y}_t &= \\mathbf{C} \\mathbf{z}_t + \\mathbf{v}_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{w}_t &\\sim \\mathcal{N}(0, \\mathbf{Q}) \\\\\\\\\n",
    "\\mathbf{v}_t &\\sim \\mathcal{N}(0, \\mathbf{R})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This synthetic setup mimics common situations in neuroscience and time-series analysis, where observed data are noisy and high-dimensional, but governed by low-dimensional latent dynamics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d514c08ff941afa5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T19:42:42.843383Z",
     "start_time": "2025-07-13T19:42:42.800797Z"
    },
    "id": "d514c08ff941afa5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Simulation parameters\n",
    "n_neurons = 50\n",
    "n_trials = 1000\n",
    "n_time_bins = 50\n",
    "n_samples = 5\n",
    "\n",
    "omega, rho = 3.14 / 8.0, 0.97\n",
    "mean_fn = utils.SpiralDynamics(omega, rho)\n",
    "\n",
    "C = torch.nn.Linear(2, n_neurons, device=\"cpu\").requires_grad_(False)\n",
    "# C = utils.FanInLinear(2, n_neurons, device=\"cpu\").requires_grad_(False)\n",
    "C.bias.data = torch.zeros_like(C.weight[:, 0])\n",
    "\n",
    "Q_diag = 3e-2 * torch.ones(2)\n",
    "Q_0_diag = 1.0 * torch.ones(2)\n",
    "# R_diag = 0.8 * torch.ones(n_neurons)\n",
    "R_diag = 0.5 + 0.5 * torch.rand(n_neurons)\n",
    "m_0 = torch.zeros(2)\n",
    "\n",
    "z = utils.sample_gauss_z(mean_fn, Q_diag, m_0, Q_0_diag, n_trials, n_time_bins)\n",
    "y = C(z) + torch.sqrt(R_diag) * torch.randn_like(C(z))\n",
    "\n",
    "# Split data\n",
    "n_valid = n_trials // 3\n",
    "n_train = n_trials - n_valid\n",
    "y_train, z_train = y[:n_train], z[:n_train]\n",
    "y_valid, z_valid = y[n_train:], z[n_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2438ba49821560",
   "metadata": {
    "id": "3e2438ba49821560"
   },
   "source": [
    "## Visualize Simulated Data\n",
    "### Single trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a54456e94c584a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T19:42:43.897940Z",
     "start_time": "2025-07-13T19:42:43.780605Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "33a54456e94c584a",
    "outputId": "6e4bef17-3d2d-4b49-d9d0-7e72cb18b4da",
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(z[0, :, 0], label=\"Latent dim 1\")\n",
    "plt.plot(z[0, :, 1], label=\"Latent dim 2\")\n",
    "plt.legend()\n",
    "plt.title(\"Ground Truth Latent Trajectory (Trial 0)\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Latent Value\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e865c88a3ea65eae",
   "metadata": {
    "id": "e865c88a3ea65eae"
   },
   "source": [
    "### Multiple trials\n",
    "Since this is a 2D example, we can look at multiple trajectories overlayed one another in state-space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f526da826593c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T19:42:45.836297Z",
     "start_time": "2025-07-13T19:42:45.086855Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "11f526da826593c",
    "outputId": "2bf0a50f-bded-46f4-a288-6e7b6afcb360",
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots()\n",
    "plot_utils.plot_two_d_vector_field(mean_fn, axs)\n",
    "\n",
    "for i in range(10):\n",
    "    axs.plot(z[i, :, 0], z[i, :, 1], linewidth=0.5)\n",
    "    if i == 0:\n",
    "        axs.scatter(z[i, 0, 0], z[i, 0, 1], marker='x', label='start')\n",
    "        axs.scatter(z[i, -1, 0], z[i, -1, 1], marker='o', label='end')\n",
    "    else:\n",
    "        axs.scatter(z[i, 0, 0], z[i, 0, 1], marker='x')\n",
    "        axs.scatter(z[i, -1, 0], z[i, -1, 1], marker='o')\n",
    "\n",
    "axs.legend()\n",
    "axs.set_title(\"Sample trajectories\")\n",
    "axs.set_xlabel(\"dim 1\")\n",
    "axs.set_ylabel(\"dim 2\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e018c7e6175e0d7",
   "metadata": {
    "id": "2e018c7e6175e0d7"
   },
   "source": [
    "## 1: Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is a classical linear method that finds directions of maximum variance in the data. While simple and efficient, it doesn't account for time or observation noise, and assumes the entire dataset lies on a linear subspace.  Still, PCA often works surprisingly well as a baseline.\n",
    "\n",
    "PCA can be related to a limiting case of a linear and Gaussian model where data is generated according to,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{z} &\\sim \\mathcal{N}(0, \\mathbf{I}) \\\\\\\\\n",
    "\\mathbf{y} \\mid \\mathbf{z} &\\sim \\mathcal{N}(\\mathbf{C} \\mathbf{z}, \\sigma^2 \\mathbf{I})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where we take,\n",
    "\n",
    "$$\n",
    "\\sigma^2 \\rightarrow 0\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86637836ad60c5f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T19:42:47.411169Z",
     "start_time": "2025-07-13T19:42:47.319352Z"
    },
    "id": "86637836ad60c5f9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(y_train.reshape(-1, n_neurons))\n",
    "eig_vec = pca.components_\n",
    "m_pca = pca.transform(y_valid.reshape(-1, n_neurons))\n",
    "m_pca = torch.tensor(m_pca.reshape(n_valid, n_time_bins, -1), dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b417a5baa5623752",
   "metadata": {
    "id": "b417a5baa5623752"
   },
   "source": [
    "Let's visualize the latent trajectories and the directions of maximum variance in the observed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393209692f73854c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T19:42:48.463374Z",
     "start_time": "2025-07-13T19:42:48.355149Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "393209692f73854c",
    "outputId": "3b78cdee-245f-4b89-ffb4-54361f341c43",
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Line plot\n",
    "axs[0].set_title('latent trajectory (dim 1)')\n",
    "axs[0].set_box_aspect(0.6)\n",
    "axs[0].plot(m_pca[0, :, 0], label='pca')\n",
    "axs[0].plot(z_valid[0, :, 0], label='true')\n",
    "\n",
    "# Imshow - horizontally stretched\n",
    "axs[1].imshow(eig_vec.T, aspect=0.1)\n",
    "plot_utils.remove_axs_fluff(axs[1])\n",
    "axs[1].set_title('eigenvectors')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437c623195f467f6",
   "metadata": {
    "id": "437c623195f467f6"
   },
   "source": [
    "Each column on the right is a particular 'principal component' and can be considered a dominant mode of instantaneous neural population activity; the left most column is the pattern of neural population activity that explains the most variance in the observed data.\n",
    "\n",
    "But is PCA doing a good job here at recovering the low-dimensional structure underlying the observed data? From the plot it looks like it's not -- however, this is because we have the freedom to rotate and scale the latent space arbitrarily.  Lets align these pca inferred 'latent trajectories' to the ground truth data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0ac10bb042f1c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T19:42:50.212560Z",
     "start_time": "2025-07-13T19:42:50.109563Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "id": "4b0ac10bb042f1c1",
    "outputId": "34cb86b6-395c-4088-aaea-fb25d6ac2171",
    "tags": []
   },
   "outputs": [],
   "source": [
    "rot_pca, m_rot_pca = align_latent_variables(z_valid, m_pca)\n",
    "\n",
    "fig, axs = plt.subplots(1, 1)\n",
    "\n",
    "# Line plot\n",
    "axs.set_title('rotated latent trajectory (dim 1)')\n",
    "axs.set_box_aspect(0.6)\n",
    "axs.plot(m_rot_pca[0, :, 0], label='pca rotated')\n",
    "axs.plot(z_valid[0, :, 0], label='true')\n",
    "\n",
    "axs.legend()\n",
    "axs.set_xlabel('time')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3f1f71ef67032f",
   "metadata": {
    "id": "dc3f1f71ef67032f"
   },
   "source": [
    "looks better, but still not great -- let's examine a slightly more sophisticated statistical method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5245b376b3e70ec",
   "metadata": {
    "id": "d5245b376b3e70ec"
   },
   "source": [
    "## 2: Factor Analysis\n",
    "\n",
    "Factor Analysis (FA) is another type of probabilistic generative model. It models the noise in each observation dimension and finds latent variables that explain shared structure across variables.  However, FA still treats each time point independently, ignoring dynamics entirely. The corresponding generative model for FA is,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{z} &\\sim \\mathcal{N}(0, \\mathbf{Q}) \\\\\\\\\n",
    "\\mathbf{y}\\mid \\mathbf{z} &\\sim \\mathcal{N}(\\mathbf{C} \\mathbf{z}, \\mathbf{R})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "and the posterior, by Bayes' rule is, $p(\\mathbf{z} \\mid \\mathbf{y}) \\propto p(\\mathbf{y}\\mid \\mathbf{z}) p(\\mathbf{z})$, which can be found analytically through some Gaussian calculus as,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(\\mathbf{z}\\mid \\mathbf{y}) &= \\mathcal{N}(\\mathbf{m}, \\mathbf{P})\\\\\\\\\n",
    "\\mathbf{P}^{-1} &= \\mathbf{Q}^{-1} + \\mathbf{C}^\\top \\mathbf{R}^{-1} \\mathbf{C}\\\\\\\\\n",
    "\\mathbf{m} &= \\mathbf{P} \\mathbf{C}^\\top \\mathbf{R}^{-1} \\mathbf{y}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Let's compute posterior statistics given data and then use them to draw samples from the posterior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df29d9cabe5f99ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T19:42:54.082111Z",
     "start_time": "2025-07-13T19:42:54.071066Z"
    },
    "id": "df29d9cabe5f99ed",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# following the equations, find the precision and mean\n",
    "J_fa = (C.weight.mT / R_diag) @ C.weight + torch.diag(1 / Q_diag)\n",
    "J_fa_chol = torch.linalg.cholesky(J_fa)\n",
    "P_fa_chol = triangular_inverse(J_fa_chol).mT\n",
    "m_fa = chol_bmv_solve(J_fa_chol, bmv(C.weight.mT, (y_valid - C.bias)))\n",
    "z_fa = m_fa.unsqueeze(0) + bmv(P_fa_chol, torch.randn((n_samples, n_valid, n_time_bins, cfg.n_latents)))\n",
    "\n",
    "# don't forget to align!\n",
    "rot_fa, m_rot_fa = align_latent_variables(z_valid, m_fa)\n",
    "z_rot_fa = bmv(rot_fa, z_fa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a6e4de821358a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T19:42:54.742962Z",
     "start_time": "2025-07-13T19:42:54.564908Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "c0a6e4de821358a0",
    "outputId": "4269c390-63cb-42b5-87f9-1ad431f34caa",
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(12, 5))\n",
    "\n",
    "for d in range(2):\n",
    "    axs[d].set_title(f'rotated latent trajectory (dim {d})')\n",
    "    axs[d].set_box_aspect(0.2)\n",
    "\n",
    "    for s in range(n_samples):\n",
    "        axs[d].plot(z_rot_fa[s, 0, :, d], linewidth=0.5, color='gray')\n",
    "\n",
    "    axs[d].plot(m_rot_fa[0, :, d], label='pca rotated')\n",
    "\n",
    "    axs[d].plot(z_valid[0, :, d], label='true')\n",
    "    axs[d].legend()\n",
    "    axs[d].set_xlabel('time')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4171f018b34938f5",
   "metadata": {
    "id": "4171f018b34938f5"
   },
   "source": [
    "looks much better than pca! factor analysis is better able to handle data with a higher SNR because the additional observation uncertainty is accounted for in the generative model. however, the ground truth trajectory is fairly smooth but samples from our posterior aren't -- factor analysis cannot account for the temporal structure underlying the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3134efc9ded8d7d",
   "metadata": {
    "id": "b3134efc9ded8d7d"
   },
   "source": [
    "## 3: Kalman Filtering and Smoothing\n",
    "\n",
    "Now, we'll account for temporal structure in the data by explicitly accounting for dynamics in the generative model -- specifically, we consider a probabilistic generative model where the latent state $\\mathbf{z}_t$ evolves according to a linear stochastic difference equation and each observation $\\mathbf{y}_t$ is linear and noisy readout of the latent state so that much like the data was generated we have,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{z}_1 &\\sim \\mathcal{N}(0, \\mathbf{Q}_0) \\\\\\\\\n",
    "\\mathbf{z}_t &= \\mathbf{A} \\mathbf{z}_{t-1} + \\mathbf{w}_t \\\\\\\\\n",
    "\\mathbf{y}_t &= \\mathbf{C} \\mathbf{z}_t + \\mathbf{v}_t\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{w}_t &\\sim \\mathcal{N}(0, \\mathbf{Q}) \\\\\\\\\n",
    "\\mathbf{v}_t &\\sim \\mathcal{N}(0, \\mathbf{R})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The Kalman filter is a recursive algorithm for calculating the statistics of the posterior 'filtering' distribution, which by linearity and Gaussianity of the system will also be Gaussian, which we specify by,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(\\mathbf{z}_t\\mid \\mathbf{y}_{1:t}) &= \\mathcal{N}(\\breve{\\mathbf{m}}_t, \\breve{\\mathbf{P}}_t)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "From these statistics, and a new observation, $\\mathbf{y}_{t+1}$, we want to update our posterior belief about $\\mathbf{z}_{t+1}$.  The great thing is that Baye's rule tells us exactly how to do this, since\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    p(\\mathbf{z}_{t+1} \\mid \\mathbf{y}_{1:t+1}) &\\propto p(\\mathbf{y}_{t+1} \\mid \\mathbf{z}_{t+1}) p(\\mathbf{z}_{t+1} \\mid \\mathbf{y}_{1:t})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We know how to do this Gaussian calculus, but, we don't know $p(\\mathbf{z}_{t+1}\\mid \\mathbf{y}_{1:t})$, so lets find that first using quantities we already know,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(\\mathbf{z}_{t+1}\\mid \\mathbf{y}_{1:t}) &= \\int p(\\mathbf{z}_{t+1}, \\mathbf{z}_t\\mid \\mathbf{y}_{1:t}) \\, d \\mathbf{z}_t \\\\\\\\\n",
    " \\text{} &= \\int p(\\mathbf{z}_{t+1} \\mid  \\mathbf{z}_t) p(\\mathbf{z}_t \\mid \\mathbf{y}_{1:t}) \\, d\\mathbf{z}_t \\\\\\\\\n",
    "\\text{} &= \\mathcal{N}(\\bar{\\mathbf{m}}_{t+1}, \\bar{\\mathbf{P}}_{t+1})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\bar{\\mathbf{m}}_{t+1} &= \\mathbf{A} \\breve{\\mathbf{m}}_t\\\\\\\\\n",
    "\\bar{\\mathbf{P}}_{t+1} &= \\mathbf{A} \\breve{\\mathbf{P}}_t \\mathbf{A}^{\\top} + \\mathbf{Q}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Notice that the predictive distribution mean and covariance are an affine combination of the filtered mean and covariance respectively.  Finally, we return to our posterior update equation and some Gaussian calculus again,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    p(\\mathbf{z}_{t+1} \\mid \\mathbf{y}_{1:t+1}) &= \\mathcal{N}(\\mathbf{m}_{t+1}, \\mathbf{P}_{t+1})\\\\\\\\\n",
    "    \\mathbf{P}_{t+1}^{-1} &= \\mathbf{Q}^{-1} + \\mathbf{C}^\\top \\mathbf{R}^{-1} \\mathbf{C}\\\\\\\\\n",
    "    \\mathbf{m}_{t+1} &= \\mathbf{P}_{t+1} (\\bar{\\mathbf{P}}_{t+1}^{-1} \\bar{\\mathbf{m}}_{t+1} + \\mathbf{C}^\\top \\mathbf{R}^{-1} \\mathbf{y}_{t+1})\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe6c46cc2353bdf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T19:43:01.931955Z",
     "start_time": "2025-07-13T19:43:01.867403Z"
    },
    "id": "8fe6c46cc2353bdf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "h_update = bmv(C.weight.T, (y_valid - C.bias) / R_diag)\n",
    "J_update = (C.weight.T / R_diag) @ C.weight\n",
    "J_update = J_update.expand(y_valid.shape[0], n_time_bins, cfg.n_latents, cfg.n_latents)\n",
    "\n",
    "m_f, P_f, m_p, P_p = kalman_information_filter(h_update, J_update, mean_fn.A, Q_diag, m_0, Q_0_diag)\n",
    "m_s, P_s, P_tp1_t_s, z_s = rts_smoother(m_p, P_p, m_f, P_f, mean_fn.A, n_samples=n_samples)\n",
    "rot_s, m_rot_s = align_latent_variables(z_valid, m_s)\n",
    "z_rot_s = bmv(rot_s, z_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722ca8ae8fa31a34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T19:43:02.719084Z",
     "start_time": "2025-07-13T19:43:02.536800Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "722ca8ae8fa31a34",
    "outputId": "46f3e7b8-da8f-482f-e509-37ac8ebe1e6c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(12, 5))\n",
    "\n",
    "for d in range(2):\n",
    "    axs[d].set_title(f'rotated latent trajectory (dim {d})')\n",
    "    axs[d].set_box_aspect(0.2)\n",
    "\n",
    "    for s in range(n_samples):\n",
    "        axs[d].plot(z_rot_s[s, 0, :, d], linewidth=0.5, color='gray')\n",
    "\n",
    "    axs[d].plot(m_rot_s[0, :, d], label='pca rotated')\n",
    "\n",
    "    axs[d].plot(z_valid[0, :, d], label='true')\n",
    "    axs[d].legend()\n",
    "    axs[d].set_xlabel('time')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3658a0051d3cd33",
   "metadata": {
    "id": "f3658a0051d3cd33"
   },
   "source": [
    "look how much smoother the posterior samples are!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fcf7c14edb5b0b",
   "metadata": {
    "id": "99fcf7c14edb5b0b"
   },
   "source": [
    "## Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7a9062db51be50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T19:43:05.287879Z",
     "start_time": "2025-07-13T19:43:05.156143Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "id": "7d7a9062db51be50",
    "outputId": "64d7f14c-d4e2-4499-8e7b-e1122e05b9da",
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(z_valid[0, :, 0], label='Ground Truth', linewidth=2)\n",
    "plt.plot(m_rot_pca[0, :, 0], label='PCA', linestyle='--')\n",
    "plt.plot(m_rot_fa[0, :, 0], label='Factor Analysis', linestyle='--')\n",
    "plt.plot(m_rot_s[0, :, 0], label='Kalman (Smoothed)', linestyle='--')\n",
    "plt.title(\"Latent Dimension 1: True vs. Estimated\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Latent Value\")\n",
    "plt.legend()\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a5ffb05ba0af39",
   "metadata": {
    "id": "41a5ffb05ba0af39"
   },
   "source": [
    "# Learning Latent Dynamics Parameters\n",
    "\n",
    "So far, we assumed access to the true dynamics and observation parameters. But in practice, these must be estimated from data.\n",
    "\n",
    "We'll now explore two approaches for learning the parameters of a Linear Dynamical System (LDS):\n",
    "\n",
    "1. **System Identification** using the **Kalman-Ho algorithm** (a subspace method).\n",
    "2. **Expectation-Maximization (EM)** for LDS parameter learning (a probabilistic approach).\n",
    "\n",
    "We'll compare their learned state transition matrices via their eigenvalues.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cee93c263de19dd",
   "metadata": {
    "id": "7cee93c263de19dd"
   },
   "source": [
    "## Kalman-Ho System Identification\n",
    "\n",
    "The Kalman-Ho algorithm is a classic subspace identification method. It works by constructing a Hankel matrix from the observed outputs and applying an SVD to extract latent dynamics.\n",
    "\n",
    "This method is fast and often used in control and system ID applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f6b9e75a62d92d",
   "metadata": {
    "id": "89f6b9e75a62d92d"
   },
   "source": [
    "# Kalman–Ho Algorithm: Derivation and Parameter Estimation\n",
    "\n",
    "We consider the linear–Gaussian state-space model:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "z_{t+1} &= A z_t + w_t, \\quad w_t \\sim \\mathcal{N}(0, Q) \\\\\n",
    "y_t &= C z_t + v_t, \\quad v_t \\sim \\mathcal{N}(0, R)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We assume stationarity: $z_t \\sim \\mathcal{N}(0, P_\\infty)$, where $P_\\infty$ satisfies the discrete Lyapunov equation:\n",
    "\n",
    "$$\n",
    "P_\\infty = A P_\\infty A^\\top + Q\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Estimate Output Covariances\n",
    "\n",
    "Given output sequences $\\{ y_t \\}$, compute empirical autocovariances:\n",
    "\n",
    "$$\n",
    "\\Gamma_k := \\mathbb{E}[y_{t+k} y_t^\\top] \\approx \\frac{1}{T - k} \\sum_{t=1}^{T-k} y_{t+k} y_t^\\top, \\quad k = 0, 1, \\dots, K\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2: Build the Hankel Matrix\n",
    "\n",
    "Construct the block Hankel matrix:\n",
    "\n",
    "$$\n",
    "H = \\begin{bmatrix}\n",
    "\\Gamma_1 & \\Gamma_2 & \\cdots & \\Gamma_k \\\\\n",
    "\\Gamma_2 & \\Gamma_3 & \\cdots & \\Gamma_{k+1} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\Gamma_j & \\Gamma_{j+1} & \\cdots & \\Gamma_{j+k-1}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3: Low-Rank Factorization via SVD\n",
    "\n",
    "Perform singular value decomposition on the Hankel matrix:\n",
    "\n",
    "$$\n",
    "H \\approx U \\Sigma V^\\top\n",
    "$$\n",
    "\n",
    "Extract the rank-$d$ approximation, and define:\n",
    "\n",
    "$$\n",
    "\\mathcal{O} := U_d \\Sigma_d^{1/2}, \\quad \\mathcal{C} := \\Sigma_d^{1/2} V_d^\\top\n",
    "$$\n",
    "\n",
    "Then:\n",
    "\n",
    "- $C \\approx \\mathcal{O}_{\\text{first block}}$\n",
    "- $B \\approx \\mathcal{C}_{\\text{first block}}$\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4: Estimate the State Transition Matrix $A$\n",
    "\n",
    "Using shift-invariance of the observability matrix:\n",
    "\n",
    "- Let $\\mathcal{O}_{\\text{top}}$ be all but the last block row\n",
    "- Let $\\mathcal{O}_{\\text{bottom}}$ be all but the first block row\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "A \\approx \\mathcal{O}_{\\text{bottom}}^\\dagger \\mathcal{O}_{\\text{top}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Step 5: Estimate Process Noise Covariance $Q$\n",
    "\n",
    "Using the fact that $B \\approx$ Cholesky-like factor of $Q$, estimate:\n",
    "\n",
    "$$\n",
    "Q \\approx B B^\\top\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Step 6: Solve for Stationary Covariance $P_\\infty$\n",
    "\n",
    "Solve the discrete Lyapunov equation:\n",
    "\n",
    "$$\n",
    "P_\\infty = A P_\\infty A^\\top + Q\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Step 7: Estimate Observation Noise Covariance $R$\n",
    "\n",
    "Use the identity from autocovariance at lag 0:\n",
    "\n",
    "$$\n",
    "\\Gamma_0 = C P_\\infty C^\\top + R\n",
    "$$\n",
    "\n",
    "Solve for:\n",
    "\n",
    "$$\n",
    "R \\approx \\Gamma_0 - C P_\\infty C^\\top\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "The Kalman–Ho algorithm enables non-iterative identification of the state-space model:\n",
    "\n",
    "- Recover $A$, $B$, $C$ from SVD of Hankel matrix\n",
    "- Estimate $Q$ from $B B^\\top$\n",
    "- Solve for $P_\\infty$ via Lyapunov equation\n",
    "- Estimate $R$ from the empirical autocovariance\n",
    "\n",
    "This procedure is fully data-driven and avoids iterative inference or EM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e13713ba37ada2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T19:43:16.311697Z",
     "start_time": "2025-07-13T19:43:13.682520Z"
    },
    "id": "2e13713ba37ada2d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Construct Hankel matrix\n",
    "H_hankel = prob_utils.construct_hankel(y_valid, 50, 50)\n",
    "Gamma_0_hat = prob_utils.compute_gamma_0(y_valid.reshape(-1, n_neurons))\n",
    "\n",
    "# Estimate system matrices using Kalman-Ho\n",
    "A_hat_kh, B_hat_kh, C_hat_kh, Q_hat_kh, R_hat_kh = prob_utils.get_kalman_ho_estimates(\n",
    "    H_hankel, Gamma_0_hat, n_neurons, cfg.n_latents\n",
    ")\n",
    "R_diag_kh = torch.diag(R_hat_kh)\n",
    "Q_diag_kh = torch.diag(Q_hat_kh)\n",
    "\n",
    "# Eigenvalues of learned A\n",
    "eig_vals_kh_hat = torch.linalg.eigvals(A_hat_kh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a622155f52f4964e",
   "metadata": {
    "id": "a622155f52f4964e"
   },
   "source": [
    "Lets compare Kalman filtering with identity dynamics versus those inferred by the Ho-Kalman algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a95dd579706c931",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T19:49:07.168515Z",
     "start_time": "2025-07-13T19:49:06.998971Z"
    },
    "id": "4a95dd579706c931",
    "tags": []
   },
   "outputs": [],
   "source": [
    "hk_update = bmv(C_hat_kh.T, y_valid / R_diag_kh) # C bias is 0\n",
    "Jk_update = (C_hat_kh.T @ R_hat_kh) @ C_hat_kh\n",
    "Jk_update = Jk_update.expand(y_valid.shape[0], n_time_bins, cfg.n_latents, cfg.n_latents)\n",
    "\n",
    "m_f_hk, P_f_hk, m_p_hk, P_p_hk = kalman_information_filter(h_update, J_update, A_hat_kh, Q_diag_kh, m_0, Q_0_diag)\n",
    "m_s_hk, P_s_hk, P_tp1_t_s_hk, z_s_hk = rts_smoother(m_p_hk, P_p_hk, m_f_hk, P_f_hk, A_hat_kh, n_samples=n_samples)\n",
    "rot_s_hk, m_rot_s_hk = align_latent_variables(z_valid, m_s_hk)\n",
    "z_rot_s_hk = bmv(rot_s_hk, z_s_hk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0380e3eb8ba267",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T19:49:15.082948Z",
     "start_time": "2025-07-13T19:49:14.894235Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "ee0380e3eb8ba267",
    "outputId": "99661177-b1ce-4dbf-99d7-126bc356044f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(12, 5))\n",
    "\n",
    "for d in range(2):\n",
    "    axs[d].set_title(f'rotated latent trajectory (dim {d})')\n",
    "    axs[d].set_box_aspect(0.2)\n",
    "\n",
    "    for s in range(n_samples):\n",
    "        axs[d].plot(z_rot_s_hk[s, 0, :, d], linewidth=0.5, color='gray')\n",
    "\n",
    "    axs[d].plot(m_rot_s_hk[0, :, d], label='ho-kalman rotated')\n",
    "\n",
    "    axs[d].plot(z_valid[0, :, d], label='true')\n",
    "    axs[d].legend()\n",
    "    axs[d].set_xlabel('time')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f667f8560bbc051d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T19:50:44.061212Z",
     "start_time": "2025-07-13T19:50:43.942170Z"
    },
    "id": "f667f8560bbc051d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "R_hat_eye = torch.ones(n_neurons)\n",
    "A_hat_eye = torch.eye(cfg.n_latents)\n",
    "C_hat_eye = torch.nn.Linear(cfg.n_latents, n_neurons, bias=False, device=hk_update.device).requires_grad_(False)\n",
    "heye_update = bmv(C_hat_eye.weight.T, y_valid / R_diag) # C bias is 0\n",
    "Jeye_update = (C_hat_eye.weight.T / R_hat_eye) @ C_hat_eye.weight\n",
    "Jeye_update = Jeye_update.expand(y_valid.shape[0], n_time_bins, cfg.n_latents, cfg.n_latents)\n",
    "\n",
    "m_f_eye, P_f_eye, m_p_eye, P_p_eye = kalman_information_filter(heye_update, Jeye_update, A_hat_eye, Q_diag, m_0, Q_0_diag)\n",
    "m_s_eye, P_s_eye, P_tp1_t_s_eye, z_s_eye = rts_smoother(m_p_eye, P_p_eye, m_f_eye, P_f_eye, A_hat_eye, n_samples=n_samples)\n",
    "rot_s_eye, m_rot_s_eye = align_latent_variables(z_valid, m_s_eye)\n",
    "z_rot_s_eye = bmv(rot_s_eye, z_s_eye)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a057b3b0b0d4b418",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T19:50:44.527427Z",
     "start_time": "2025-07-13T19:50:44.328585Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "a057b3b0b0d4b418",
    "outputId": "84befa9b-7473-4f8b-e560-e43745998257",
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(12, 5))\n",
    "\n",
    "for d in range(2):\n",
    "    axs[d].set_title(f'rotated latent trajectory (dim {d})')\n",
    "    axs[d].set_box_aspect(0.2)\n",
    "\n",
    "    for s in range(n_samples):\n",
    "        axs[d].plot(z_rot_s_eye[s, 0, :, d], linewidth=0.5, color='gray')\n",
    "\n",
    "    axs[d].plot(m_rot_s_eye[0, :, d], label='identity kalman rotated')\n",
    "\n",
    "    axs[d].plot(z_valid[0, :, d], label='true')\n",
    "    axs[d].legend()\n",
    "    axs[d].set_xlabel('time')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b0d0044ef5d47a",
   "metadata": {
    "id": "49b0d0044ef5d47a"
   },
   "source": [
    "## EM for LDS Parameter Estimation\n",
    "\n",
    "The EM algorithm is a probabilistic approach to estimating LDS parameters. It alternates between:\n",
    "\n",
    "- **E-step**: Inferring latent trajectories (here, using RTS smoothing).\n",
    "- **M-step**: Updating parameters to maximize the expected complete-data log-likelihood.\n",
    "\n",
    "This method can be more accurate and flexible, especially with noise or missing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a4cdcb2eb9c13e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T19:51:01.582942Z",
     "start_time": "2025-07-13T19:51:01.572291Z"
    },
    "id": "b1a4cdcb2eb9c13e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Estimate parameters using EM\n",
    "A_hat_em, C_hat_em, Q_hat_em, R_hat_em = prob_utils.em_update_batch(m_s, P_s, P_tp1_t_s, y_valid)\n",
    "\n",
    "# Eigenvalues of learned A\n",
    "eig_vals_em_hat = torch.linalg.eigvals(A_hat_em)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7d6d1e-34f0-4bbb-b889-9a252e1cff8a",
   "metadata": {
    "id": "1b7d6d1e-34f0-4bbb-b889-9a252e1cff8a"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
