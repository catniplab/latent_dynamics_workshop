{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb9ed2ff-dcea-4a93-b574-0605640f7ddc",
   "metadata": {},
   "source": [
    "# Generative perspective on State Space Models with spike train observations\n",
    "\n",
    "In this notebook, we will play around with parameters of a state space model and generate various spike trains.\n",
    "These population activity patterns will be fun to look at. (hopefully)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63ec82d-8894-4fcf-aa88-1d83ed5c924d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set a default figure size for matplotlib figures (using 1 + golden ratio)\n",
    "plt.rcParams['figure.figsize'] = (10 * np.array([1, 1 / (1 + (np.sqrt(5)+ 1)/2)])).tolist()\n",
    "# set a default font size for matplotlib figures\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636107d5",
   "metadata": {},
   "source": [
    "## Neural Manifolds and Causality\n",
    "\n",
    "Neurons cause the coordinated activity pattern that we experimentally observe. However, with the current experimental technology, we do not yet have enough data to recover the full spiking neural network faithfully.\n",
    "\n",
    "Fortunately, neural recordings have a lot of spatial structure that restricts activity to a low-dimensional manifold.\n",
    "Moreover, neural recordings have a lot of temporal structure to define an effective \"flow\" on the manifold.\n",
    "\n",
    "Therefore, to analyze the neural data, we could focus on the effective collective behavior of the population reflected in neural recordings.\n",
    "We assume we have access to the low-dimensional population state via the partially observed neurons.\n",
    "Think of the neurons as noisy measurement devices that are coupled with the thinig we really want to measure, the neural population state.\n",
    "\n",
    "This means, we can define an observation model: $p(y(t) \\mid x(t))$ where $y(t)$ is the neural data and $x(t)$ is the latent (i.e. hidden or unobserved) neural state. $x(t)$ lives on the coordinate system that parametrizes the neural manifold or its embedding space.\n",
    "\n",
    "This may may look acausal, but it makes sense in the statistical sense. Using this model does not assume that the neurons are not fundamentally generating the latent state dynamics, it's merely a methodological necessity.\n",
    "\n",
    "For educational purposes, it is useful to generate spike trains as if this acausal model is true.\n",
    "It allows to gain intuition about what the statistical model expects the observed data to be if the assumptions hold up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c450d2fb-e3cd-459a-9b5f-934856891aed",
   "metadata": {},
   "source": [
    "## A simple 1-D latent (neural) trajectory\n",
    "\n",
    "For illustration, we will use a sinusoid as the 1-D latent trajectory.\n",
    "$$ x(t) = sin(2\\pi f\\cdot t) $$\n",
    "In this example, $x(t)$ represents the (instantaneous) state of the neural population of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5c27a9-be0d-4ab0-be3c-7b798ad3d728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate a simple latent trajectory\n",
    "nT = 1000  # number of time points (samples) in the simulated trajectory\n",
    "T = 10 # duration of the trajectory in seconds\n",
    "frq = 0.3 # frequency of the sinusoid in Hz\n",
    "tr = np.linspace(0, T, nT) # time range\n",
    "dt = tr[1] - tr[0] # time step in seconds\n",
    "x = np.sin(2 * np.pi * frq * tr) # generate a sinusoid over time\n",
    "fig = plt.figure(); plt.plot(tr, x); plt.title('1-D latent trajectory'); plt.xlabel('time (s)');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb978cc-72bc-4ede-8cde-cb5101d81177",
   "metadata": {},
   "source": [
    "## One Poisson neuron driven by the latent process\n",
    "\n",
    "We will generate spike trains from a *Poisson neuron*, which is just an inhomogeneous Poisson process with a time varying firing rate function $\\lambda(t)$.\n",
    "The spike count $y(t)$ in a small time bin of size $\\Delta$ is distributed as the Poisson distribution:\n",
    "$$ y(t) \\sim \\text{Poisson}(\\Delta\\lambda(t)) $$\n",
    "\n",
    "Importantly, the firing rate will be a function of $x(t)$, but not of past $x$ nor past $y$.\n",
    "$$ \\lambda(t) = g(x(t)) $$\n",
    "The only constraint for $g(\\cdot)$ is that the resulting firing rate has to be non-negative.\n",
    "A mathematically convenient function is the exponential function and relates to the spike-response model (Gerstner & Kistler 2002).\n",
    "\n",
    "$$ \\lambda(t) = \\exp(a x(t) + b) = \\exp(b)\\exp(a x(t)) $$\n",
    "\n",
    "- Gerstner, W., & Kistler, W. M. (2002). Spiking Neuron Models. Cambridge University Press. https://doi.org/10.1017/cbo9780511815706"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8775d7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the log-linear Poisson neuron's tuning curve\n",
    "# y-axis is the firing rate, x-axis is the signal to be encoded\n",
    "a = 5\n",
    "b = -3\n",
    "lam = np.exp(a * x + b)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(x, lam)\n",
    "plt.xlabel('x (latent process)')\n",
    "plt.ylabel('Firing rate λ(x)')\n",
    "plt.title('Log-linear Poisson neuron tuning curve')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e489539f-e458-49de-91a2-217f655d6ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.random.poisson(lam*dt)\n",
    "\n",
    "plt.figure(figsize=(10, 2))\n",
    "plt.plot(tr, lam, label='firing rate');\n",
    "plt.eventplot(np.nonzero(y)[0]/nT*T, lw=0.5, color='k', label='spikes')\n",
    "plt.xlim(0, T); plt.xlabel('time'); plt.yticks([]); plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee64c02-2a4a-4594-9775-51634ace2794",
   "metadata": {},
   "source": [
    "## A population of Poisson neurons driven by a common 1-D latent process\n",
    "\n",
    "We can have more than one neuron that's driven by the same latent process.\n",
    "This way, we an have more observation dimensions than the latent state space dimension.\n",
    "Let's given them a random amount of \"drive\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57877799-aa62-41c3-86e7-05c90f71b35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nNeuron = 200\n",
    "C = 2 * np.random.randn(nNeuron)\n",
    "b = -2.0 + np.random.rand(nNeuron,1)\n",
    "lam = np.exp(np.outer(C, x) + b)\n",
    "y = np.random.poisson(lam*dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77fc44d-149c-4fbd-a65c-3a2aa4ea2100",
   "metadata": {},
   "source": [
    "We can make spike raster plot. But since we know the amount of drive (each value in $C$), we can sort the neurons accordingly as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c66547-3833-40c7-9375-e13265f15489",
   "metadata": {},
   "outputs": [],
   "source": [
    "cidx = np.argsort(C)\n",
    "\n",
    "raster = []\n",
    "rasterSorted = []\n",
    "for k in range(nNeuron):\n",
    "    raster.append(np.nonzero(y[k,:])[0]/nT*T)\n",
    "    rasterSorted.append(np.nonzero(y[cidx[k],:])[0]/nT*T)\n",
    "\n",
    "plt.subplots(1,2, figsize=(10, 4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.eventplot(raster, lw=0.5, color='k', label='spikes')\n",
    "plt.xlim(0, T); plt.xlabel('time'); plt.yticks([]); plt.title('raster plot'); plt.ylabel('neurons');\n",
    "plt.subplot(1,2,2)\n",
    "plt.eventplot(rasterSorted, lw=0.5, color='k', label='spikes')\n",
    "plt.xlim(0, T); plt.xlabel('time'); plt.yticks([]); plt.title('raster plot (again)'); plt.ylabel('sorted neurons');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74846f2d-1c13-48a5-b278-736348c55aa9",
   "metadata": {},
   "source": [
    "## 2D latent space example\n",
    "\n",
    "Here we will build a 2D manifold with two independent processes.\n",
    "The first latent dimension will be same as above, but we will add $x_2(t)$ as a sawtooth function:\n",
    "$$ x_2(t) = t \\,\\, \\text{mod} \\, 1 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0a34da-dda3-4c8f-ad56-eb1cccabe19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = 1.5 * ((tr % 1) - 0.5)\n",
    "X = np.vstack([x, x2]) # (latent dim) x (time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c58c0bc-2d13-4cd8-aa4c-f2246e87ad7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(2,1,figsize=(10,4))\n",
    "plt.subplot(2,1,1);\n",
    "plt.plot(tr, x ); plt.ylabel('first latent dim'); plt.xlabel('time')\n",
    "plt.subplot(2,1,2);\n",
    "plt.plot(tr, x2); plt.ylabel('second latent dim'); plt.xlabel('time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f9ec7e-0f57-4925-b4d3-502d981264bc",
   "metadata": {},
   "source": [
    "Now that we have more than 1 latent dimension, we face a choice of how neurons relate to each of the latent dimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27ae3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f141047b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/catniplab/neurofisherSNR.git\n",
    "#!pip install ./neurofisherSNR\n",
    "\n",
    "from neurofisherSNR.optimize import optimize_C\n",
    "\n",
    "nNeuron = 100\n",
    "dLatent = X.shape[0]\n",
    "snr_db = -1\n",
    "\n",
    "C = 2 * np.random.randn(dLatent, nNeuron) # random projection\n",
    "b = np.random.randn(nNeuron) - np.log(0.01)\n",
    "\n",
    "C, b, snr = optimize_C(X.T, C, b, tgt_rate_per_bin=0.01, tgt_snr=snr_db, max_rate_per_bin=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468d1ce3-bf62-4295-8d6c-e1ab3616023a",
   "metadata": {},
   "source": [
    "### Random projection observation\n",
    "\n",
    "Random projection assumes that each neuron is deriven by all the latent dimensions with a random amount.\n",
    "Under this assumption, the neural manifold is likely oblique to the axes, i.e., the neuron will be modulated by changes in any direction in the latent state space.\n",
    "Theoretical analysis of [Gao & Ganguli 2015] assumes random projections and showed that not many neurons need to be sampled (observed) to recover the manifold structure.\n",
    "In addition, the so-called *mixed-selectivity* [Tye et al. 2024] appears as a result.\n",
    "\n",
    "- Gao, P., & Ganguli, S. (2015). On Simplicity and Complexity in the Brave New World of Large-Scale Neuroscience. Current Opinion in Neurobiology, 32, 148–155.\n",
    "- Tye, K. M., Miller, E. K., Taschbach, F. H., Benna, M. K., Rigotti, M., & Fusi, S. (2024). Mixed selectivity: Cellular computations for complexity. Neuron, 112(14), 2289–2303. https://doi.org/10.1016/j.neuron.2024.04.017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65628119-5f7f-4686-a342-0958f4386291",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dLatent = X.shape[0]\n",
    "#C = 2 * np.random.randn(nNeuron, dLatent) # random projection\n",
    "lam = np.exp(X.T @ C + b)\n",
    "y = np.random.poisson(lam*dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d719f6ae-6e81-4f5b-af6e-86196b99940e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cidx1 = np.lexsort((C[:,0], C[:,1]), axis=0)\n",
    "cidx2 = np.lexsort((C[:,1], C[:,0]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfa3cb3-a58e-43d3-bea5-c288693a9e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "raster = []; rasterSorted1 = []; rasterSorted2 = []\n",
    "for k in range(nNeuron):\n",
    "    raster.append(np.nonzero(y[k,:])[0]/nT*T)\n",
    "    rasterSorted1.append(np.nonzero(y[cidx1[k],:])[0]/nT*T)\n",
    "    rasterSorted2.append(np.nonzero(y[cidx2[k],:])[0]/nT*T)\n",
    "\n",
    "plt.subplots(1,3, figsize=(10, 3))\n",
    "plt.subplot(1,3,1)\n",
    "plt.eventplot(raster, lw=0.5, color='k', label='spikes')\n",
    "plt.xlim(0, T); plt.xlabel('time'); plt.yticks([]); plt.title('raster plot'); plt.ylabel('neurons');\n",
    "plt.subplot(1,3,2)\n",
    "plt.eventplot(rasterSorted1, lw=0.5, color='k', label='spikes')\n",
    "plt.xlim(0, T); plt.xlabel('time'); plt.yticks([]); plt.title('raster plot (1)'); plt.ylabel('sorted neurons');\n",
    "plt.subplot(1,3,3)\n",
    "plt.eventplot(rasterSorted2, lw=0.5, color='k', label='spikes')\n",
    "plt.xlim(0, T); plt.xlabel('time'); plt.yticks([]); plt.title('raster plot (2)'); plt.ylabel('sorted neurons');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890bbe98-bb8c-404d-9b07-dff7d2a890cd",
   "metadata": {},
   "source": [
    "### Axis aligned observation\n",
    "\n",
    "Biologists have long loved neurons that are tuned specifically for a particular feature but not modulated by others.\n",
    "In our context, the neurons will be either driven by the first dimension or the second dimension of the latent process.\n",
    "Recent paper argues that this is optimal [Whittington et al. 2022].\n",
    "\n",
    " - Whittington, J. C. R., Dorrell, W., Ganguli, S., & Behrens, T. E. J. (2022). Disentangling with Biological Constraints: A Theory of Functional Cell Types. In arXiv [q-bio.NC]. arXiv. http://arxiv.org/abs/2210.01768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534a8046-8d7c-41cf-925f-0c6456ad6c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 2.0 * np.random.randn(nNeuron, dLatent)\n",
    "b = -2.0 + np.random.rand(nNeuron,1)\n",
    "bidx = np.random.rand(nNeuron) < 0.5\n",
    "C[bidx, 0] = 0\n",
    "C[~bidx, 1] = 0\n",
    "b[bidx] += 1.5 # boost the firing rate a bit for the 2nd latent dim\n",
    "lam = np.exp(C @ X + b)\n",
    "y = np.random.poisson(lam*dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac5362f-7904-49bf-a788-2d6cd019037a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cidx = np.lexsort((C[:,1], C[:,0]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88bb2e4-b593-44d7-b3f6-12de43701f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "raster = []\n",
    "rasterSorted = []\n",
    "for k in range(nNeuron):\n",
    "    raster.append(np.nonzero(y[k,:])[0]/nT*T)\n",
    "    rasterSorted.append(np.nonzero(y[cidx[k],:])[0]/nT*T)\n",
    "\n",
    "plt.subplots(1,2, figsize=(10, 4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.eventplot(raster, lw=0.5, color='k', label='spikes')\n",
    "plt.xlim(0, T); plt.xlabel('time'); plt.yticks([]); plt.title('raster plot'); plt.ylabel('neurons');\n",
    "plt.subplot(1,2,2)\n",
    "plt.eventplot(rasterSorted, lw=0.5, color='k', label='spikes')\n",
    "plt.xlim(0, T); plt.xlabel('time'); plt.yticks([]); plt.title('raster plot (again)'); plt.ylabel('sorted neurons');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0374ba06",
   "metadata": {},
   "source": [
    "# Signal-to-Noise Ratio of population spike trains\n",
    "\n",
    "https://github.com/catniplab/neuro-fisher\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bd59c8-85c3-458a-ba3b-24dce4e20287",
   "metadata": {},
   "source": [
    "## Some dynamical law governing the latent states\n",
    "\n",
    "So far, the latent states were given and not generated in a Markovian manner, that is, given the current state $x(t)$, the future states were not a function of $x(t-1)$ (or further past).\n",
    "In general, a dynamical law can be represented as a *dynamical system* or an ordinary differential equation (ODE):\n",
    "$$ \\frac{dx}{dt} = \\dot{x} = f(x(t)) $$\n",
    "where $f()$ is a smooth function that represents the vector field."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79937bc",
   "metadata": {},
   "source": [
    "### van der Pol oscillator\n",
    "Van der pol oscillator is defined as 2D dimensional first order differential equations:\n",
    "    $$ \\dot{x} = y$$\n",
    "    $$ \\dot{y} = \\mu(1-x^2)y -x $$\n",
    "\n",
    "For our simulations we take a discrete time grid for convenience.\n",
    "We use an Euler integration of a Van der Pol oscillator with noisy transitions with $\\mu=1.5$, $\\tau_1=0.1$, $\\tau_2=0.1$, and $\\sigma=0.1$:\n",
    "\n",
    "$$ x_{t+1,1} = x_{t,1} + \\tau_1^{-1} \\Delta x_{t,2} + \\sigma \\epsilon$$\n",
    "$$ x_{t+1,2} = x_{t,2} + \\tau_2^{-1} \\Delta(\\mu (1-x_{t,1})^2 x_{t,2} - x_{t,1}) + \\sigma \\epsilon$$\n",
    "\n",
    "For the sake of brevity in the notebook, the generation code is provided in `code_pack/genereate_vdp_data.py` and we only load the saved data here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21e424d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import scipy\n",
    "import scipy.ndimage\n",
    "\n",
    "from code_pack.plotting import plot_two_d_vector_field_from_data, raster_to_events\n",
    "from code_pack.generate_vdp_data import generate_van_der_pol, generate_noisy_van_der_pol\n",
    "\n",
    "# loading data from ./data/vdp_noisy.h5\n",
    "file_name = \"vanderpol/data/poisson_obs.h5\"\n",
    "\n",
    "# dynamics parameters\n",
    "data = h5py.File(file_name, 'r')\n",
    "system_parameters = {}\n",
    "system_parameters['mu'] = data['mu']\n",
    "system_parameters['tau_1'] = data['tau_1']\n",
    "system_parameters['tau_2'] = data['tau_2']\n",
    "system_parameters['sigma'] = data['sigma']\n",
    "system_parameters['scale'] = np.array(data['scale'])\n",
    "\n",
    "Y = np.array(data['Y'])\n",
    "X = np.array(data['X'])\n",
    "C = np.array(data['C'])\n",
    "b = np.array(data['bias'])\n",
    "\n",
    "n_trials = Y.shape[0]\n",
    "n_latents = X.shape[2]\n",
    "n_neurons = Y.shape[2]\n",
    "n_time_bins = Y.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a279499d",
   "metadata": {},
   "source": [
    "### Visualizationing trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad83d305",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# plotting trajectories of the dataset\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5,5))\n",
    "_ = ax.plot(X[0,:,0], X[0,:,1])\n",
    "ax.scatter(X[0, 0, 0], X[0, 0, 1], marker='o', color='red', zorder=10, s=100, label='start')\n",
    "ax.scatter(X[0, -1, 0], X[0, -1, 1], marker='x', color='red', zorder=10, s=100, label='end')\n",
    "\n",
    "# system_parameters_copy = copy.deepcopy(system_parameters)\n",
    "system_parameters['sigma'] = 0.0\n",
    "dynamic_func = lambda inp : generate_noisy_van_der_pol(inp, np.array([0.0, 5e-3]), system_parameters)\n",
    "axs_range = {'x_min':-1.5, 'x_max':1.5, 'y_min':-1.5, 'y_max':1.5}\n",
    "plot_two_d_vector_field_from_data(dynamic_func, ax, axs_range)\n",
    "\n",
    "ax.legend()\n",
    "ax.set_title('sample trajectory (true state)');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca8cf43",
   "metadata": {},
   "source": [
    "### Effect of tuning function\n",
    "\n",
    "TODO: explain different inverse link functions (soft plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3ebf33",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "C_tilde = np.array(data['C_tilde'])\n",
    "idx = np.lexsort((C_tilde[:, 0], C_tilde[:, 1]), axis=0)  # sort the loading\n",
    "\n",
    "# showing the spike raster generated from noisy Vdp\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 3), sharex=True, sharey=True)\n",
    "events = raster_to_events(np.array(data['Y'])[0, :, :])\n",
    "events_softplus = raster_to_events(np.array(data['Y_softplus'])[0, :, :])\n",
    "events_axis_aligned = raster_to_events(np.array(data['Y_axis'])[0, :, idx].transpose())\n",
    "axs[0].eventplot(events, linewidths=0.5, color='k');\n",
    "axs[1].eventplot(events_softplus, linewidths=0.5, color='k');\n",
    "axs[2].eventplot(events_axis_aligned, linewidths=0.5, color='k');\n",
    "axs[0].set_title(f'exp()$');\n",
    "axs[1].set_title(f'softplus$()$');\n",
    "axs[2].set_title(f'axis aligned');\n",
    "axs[0].set_xlabel(\"Time\");\n",
    "axs[0].set_ylabel(\"Neuron\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ab1d7f",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "Now we understand better the generative process of the model. But what we are interested is the opposite direction, that is, how do we infer the model parameters given just the observations (neural data)? This is the statistical inference problem of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60578478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da76ba8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "xfads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
