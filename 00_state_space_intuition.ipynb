{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb9ed2ff-dcea-4a93-b574-0605640f7ddc",
   "metadata": {},
   "source": [
    "# Generative perspective on State Space Models with spike train observations\n",
    "\n",
    "In this notebook, we will play around with parameters of a state space model and generate various spike trains.\n",
    "These population activity patterns will be fun to look at. (hopefully)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63ec82d-8894-4fcf-aa88-1d83ed5c924d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set a default figure size for matplotlib figures (using 1 + golden ratio)\n",
    "plt.rcParams['figure.figsize'] = (10 * np.array([1, 1 / (1 + (np.sqrt(5)+ 1)/2)])).tolist()\n",
    "# set a default font size for matplotlib figures\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636107d5",
   "metadata": {},
   "source": [
    "## Neural Manifolds and Causality\n",
    "\n",
    "Neurons cause the coordinated activity pattern that we experimentally observe. However, with the current experimental technology, we do not yet have enough data to recover the full spiking neural network faithfully.\n",
    "\n",
    "Fortunately, neural recordings have a lot of spatial structure that restricts activity to a low-dimensional manifold.\n",
    "Moreover, neural recordings have a lot of temporal structure to define an effective \"flow\" on the manifold.\n",
    "\n",
    "Therefore, to analyze the neural data, we could focus on the effective collective behavior of the population reflected in neural recordings.\n",
    "We assume we have access to the low-dimensional population state via the partially observed neurons.\n",
    "Think of the neurons as noisy measurement devices that are coupled with the thinig we really want to measure, the neural population state.\n",
    "\n",
    "This means, we can define an observation model: $p(y(t) \\mid x(t))$ where $y(t)$ is the neural data and $x(t)$ is the latent (i.e. hidden or unobserved) neural state. $x(t)$ lives on the coordinate system that parametrizes the neural manifold or its embedding space.\n",
    "\n",
    "This may may look acausal, but it makes sense in the statistical sense. Using this model does not assume that the neurons are not fundamentally generating the latent state dynamics, it's merely a methodological necessity.\n",
    "\n",
    "For educational purposes, it is useful to generate spike trains as if this acausal model is true.\n",
    "It allows to gain intuition about what the statistical model expects the observed data to be if the assumptions hold up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c450d2fb-e3cd-459a-9b5f-934856891aed",
   "metadata": {},
   "source": [
    "## A simple 1-D latent (neural) trajectory\n",
    "\n",
    "For illustration, we will use a sinusoid as the 1-D latent trajectory.\n",
    "$$ x(t) = sin(2\\pi f\\cdot t) $$\n",
    "In this example, $x(t)$ represents the (instantaneous) state of the neural population of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5c27a9-be0d-4ab0-be3c-7b798ad3d728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate a simple latent trajectory\n",
    "nT = 1000  # number of time points (samples) in the simulated trajectory\n",
    "T = 10 # duration of the trajectory in seconds\n",
    "frq = 0.3 # frequency of the sinusoid in Hz\n",
    "tr = np.linspace(0, T, nT) # time range\n",
    "dt = tr[1] - tr[0] # time step in seconds\n",
    "x = np.sin(2 * np.pi * frq * tr)[:, np.newaxis]  # generate a sinusoid over time, shape [nT, 1]\n",
    "fig = plt.figure(); plt.plot(tr, x); plt.title('1-D latent trajectory'); plt.xlabel('time (s)');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb978cc-72bc-4ede-8cde-cb5101d81177",
   "metadata": {},
   "source": [
    "## One Poisson neuron driven by the latent process\n",
    "\n",
    "We will generate spike trains from a *Poisson neuron*, which is just an inhomogeneous Poisson process with a time varying firing rate function $\\lambda(t)$.\n",
    "The spike count $y(t)$ in a small time bin of size $\\Delta$ is distributed as the Poisson distribution:\n",
    "$$ y(t) \\sim \\text{Poisson}(\\Delta\\lambda(t)) $$\n",
    "\n",
    "Importantly, the firing rate will be a function of $x(t)$, but not of past $x$ nor past $y$.\n",
    "$$ \\lambda(t) = g(x(t)) $$\n",
    "The only constraint for $g(\\cdot)$ is that the resulting firing rate has to be non-negative.\n",
    "A mathematically convenient function is the exponential function and relates to the spike-response model (Gerstner & Kistler 2002).\n",
    "\n",
    "$$ \\lambda(t) = \\exp(a x(t) + b) = \\exp(b)\\exp(a x(t)) $$\n",
    "\n",
    "- Gerstner, W., & Kistler, W. M. (2002). Spiking Neuron Models. Cambridge University Press. https://doi.org/10.1017/cbo9780511815706"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8775d7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the log-linear Poisson neuron's tuning curve\n",
    "# y-axis is the firing rate, x-axis is the signal to be encoded\n",
    "a = 5\n",
    "b = -3\n",
    "lam = np.exp(a * x + b)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(x, lam)\n",
    "plt.xlabel('x (latent process)')\n",
    "plt.ylabel('Firing rate λ(x) in Hz')\n",
    "plt.title('Log-linear Poisson neuron tuning curve')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e489539f-e458-49de-91a2-217f655d6ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.random.poisson(lam*dt)\n",
    "\n",
    "plt.figure(figsize=(10, 2))\n",
    "plt.plot(tr, lam, label='firing rate');\n",
    "plt.eventplot(np.nonzero(y)[0]/nT*T, lw=0.5, color='k', label='spikes')\n",
    "plt.xlim(0, T); plt.xlabel('time'); plt.yticks([]); plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee64c02-2a4a-4594-9775-51634ace2794",
   "metadata": {},
   "source": [
    "## A population of Poisson neurons driven by a common 1-D latent process\n",
    "\n",
    "We can have more than one neuron that's driven by the same latent process.\n",
    "This way, we an have more observation dimensions than the latent state space dimension.\n",
    "Let's given them a random amount of \"drive\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57877799-aa62-41c3-86e7-05c90f71b35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nNeuron = 200\n",
    "C = 2 * np.random.randn(nNeuron, 1)\n",
    "b = -2.0 + np.random.rand(1, nNeuron)\n",
    "lam = np.exp(x @ C.T + b)\n",
    "y = np.random.poisson(lam*dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77fc44d-149c-4fbd-a65c-3a2aa4ea2100",
   "metadata": {},
   "source": [
    "We can make spike raster plot. But since we know the amount of drive (each value in $C$), we can sort the neurons accordingly as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c66547-3833-40c7-9375-e13265f15489",
   "metadata": {},
   "outputs": [],
   "source": [
    "cidx = np.argsort(C)\n",
    "\n",
    "raster = []\n",
    "rasterSorted = []\n",
    "for k in range(nNeuron):\n",
    "    raster.append(np.nonzero(y[k,:])[0]/nT*T)\n",
    "    rasterSorted.append(np.nonzero(y[cidx[k],:])[0]/nT*T)\n",
    "\n",
    "plt.subplots(1,2, figsize=(10, 4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.eventplot(raster, lw=0.5, color='k', label='spikes')\n",
    "plt.xlim(0, T); plt.xlabel('time'); plt.yticks([]); plt.title('raster plot'); plt.ylabel('neurons');\n",
    "plt.subplot(1,2,2)\n",
    "plt.eventplot(rasterSorted, lw=0.5, color='k', label='spikes')\n",
    "plt.xlim(0, T); plt.xlabel('time'); plt.yticks([]); plt.title('raster plot (again)'); plt.ylabel('sorted neurons');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490f1736",
   "metadata": {},
   "source": [
    "# Signal-to-Noise Ratio of population spike trains\n",
    "\n",
    "We will use the `neurofisherSNR` package to estimate the upper bound of the signal-to-noise ratio to see how much information the spike trains have about the latent variable per time bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d42b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/catniplab/neurofisherSNR.git\n",
    "#!pip install ./neurofisherSNR\n",
    "\n",
    "from neurofisherSNR.snr import SNR_bound_instantaneous\n",
    "from neurofisherSNR.utils import power_to_dB, power_from_dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddbabe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "SNRdb = SNR_bound_instantaneous(x, C.T, b)\n",
    "print(f\"{SNRdb:.2f} dB\")  # note that decibel is a logarithmic unit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74846f2d-1c13-48a5-b278-736348c55aa9",
   "metadata": {},
   "source": [
    "## 2D latent space example\n",
    "\n",
    "Here we will build a 2D manifold with two independent processes.\n",
    "The first latent dimension will be same as above, but we will add $x_2(t)$ as a sawtooth function:\n",
    "$$ x_2(t) = t \\,\\, \\text{mod} \\, 1 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0a34da-dda3-4c8f-ad56-eb1cccabe19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = 1.5 * ((tr % 1) - 0.5)[:, np.newaxis]\n",
    "X = np.hstack([x, x2]) # (time) x (latent dim), python likes to have the time dimension first\n",
    "dLatent = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c58c0bc-2d13-4cd8-aa4c-f2246e87ad7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(2,1,figsize=(10,4))\n",
    "plt.subplot(2,1,1);\n",
    "plt.plot(tr, x ); plt.ylabel('first latent dim'); plt.xlabel('time')\n",
    "plt.subplot(2,1,2);\n",
    "plt.plot(tr, x2); plt.ylabel('second latent dim'); plt.xlabel('time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f9ec7e-0f57-4925-b4d3-502d981264bc",
   "metadata": {},
   "source": [
    "Now that we have more than 1 latent dimension, we face a choice of how neurons relate to each of the latent dimes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468d1ce3-bf62-4295-8d6c-e1ab3616023a",
   "metadata": {},
   "source": [
    "### Random projection observation\n",
    "\n",
    "Random projection assumes that each neuron is deriven by all the latent dimensions with a random amount.\n",
    "Under this assumption, the neural manifold is likely oblique to the axes, i.e., the neuron will be modulated by changes in any direction in the latent state space.\n",
    "Theoretical analysis of [Gao & Ganguli 2015] assumes random projections and showed that not many neurons need to be sampled (observed) to recover the manifold structure.\n",
    "In addition, the so-called *mixed-selectivity* [Tye et al. 2024] appears as a result.\n",
    "\n",
    "- Gao, P., & Ganguli, S. (2015). On Simplicity and Complexity in the Brave New World of Large-Scale Neuroscience. Current Opinion in Neurobiology, 32, 148–155.\n",
    "- Tye, K. M., Miller, E. K., Taschbach, F. H., Benna, M. K., Rigotti, M., & Fusi, S. (2024). Mixed selectivity: Cellular computations for complexity. Neuron, 112(14), 2289–2303. https://doi.org/10.1016/j.neuron.2024.04.017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65628119-5f7f-4686-a342-0958f4386291",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 0.8 * np.random.randn(nNeuron, dLatent) # random projection\n",
    "b = 0.1 * np.random.randn(nNeuron) + np.log(5)\n",
    "lam = np.exp(X @ C.T + b)\n",
    "y = np.random.poisson(lam*dt)\n",
    "\n",
    "SNRdb = SNR_bound_instantaneous(X, C.T, b)\n",
    "print(f\"{SNRdb:.2f} dB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d719f6ae-6e81-4f5b-af6e-86196b99940e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cidx1 = np.lexsort((C[:,0], C[:,1]), axis=0)\n",
    "cidx2 = np.lexsort((C[:,1], C[:,0]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfa3cb3-a58e-43d3-bea5-c288693a9e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "raster = []; rasterSorted1 = []; rasterSorted2 = []\n",
    "for k in range(nNeuron):\n",
    "    raster.append(np.nonzero(y[:,k])[0]/nT*T)\n",
    "    rasterSorted1.append(np.nonzero(y[:, cidx1[k]])[0]/nT*T)\n",
    "    rasterSorted2.append(np.nonzero(y[:, cidx2[k]])[0]/nT*T)\n",
    "\n",
    "plt.subplots(1,3, figsize=(10, 3))\n",
    "plt.subplot(1,3,1)\n",
    "plt.eventplot(raster, lw=0.5, color='k', label='spikes')\n",
    "plt.xlim(0, T); plt.xlabel('time'); plt.yticks([]); plt.title('raster plot'); plt.ylabel('neurons');\n",
    "plt.subplot(1,3,2)\n",
    "plt.eventplot(rasterSorted1, lw=0.5, color='k', label='spikes')\n",
    "plt.xlim(0, T); plt.xlabel('time'); plt.yticks([]); plt.ylabel('sorted neurons');\n",
    "plt.subplot(1,3,3)\n",
    "plt.eventplot(rasterSorted2, lw=0.5, color='k', label='spikes')\n",
    "plt.xlim(0, T); plt.xlabel('time'); plt.yticks([]); plt.ylabel('sorted neurons');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890bbe98-bb8c-404d-9b07-dff7d2a890cd",
   "metadata": {},
   "source": [
    "### Axis aligned observation\n",
    "\n",
    "Biologists have long loved neurons that are tuned specifically for a particular feature but not modulated by others.\n",
    "In our context, the neurons will be either driven by the first dimension or the second dimension of the latent process.\n",
    "Recent paper argues that this is optimal [Whittington et al. 2022].\n",
    "\n",
    " - Whittington, J. C. R., Dorrell, W., Ganguli, S., & Behrens, T. E. J. (2022). Disentangling with Biological Constraints: A Theory of Functional Cell Types. In arXiv [q-bio.NC]. arXiv. http://arxiv.org/abs/2210.01768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534a8046-8d7c-41cf-925f-0c6456ad6c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bidx = np.random.rand(nNeuron) < 0.5\n",
    "C[bidx, 0] = 0\n",
    "C[~bidx, 1] = 0\n",
    "b[bidx] += 1.5 # boost the firing rate a bit for the 2nd latent dim\n",
    "lam = np.exp(X @ C.T + b)\n",
    "y = np.random.poisson(lam*dt)\n",
    "\n",
    "# for independent populations with independent latents, the SNR is the sum of the SNRs of the two populations\n",
    "SNRdb1 = SNR_bound_instantaneous(X[:,[0]], C[:,[0]].T, b)  # keep the singular rows or columns\n",
    "SNRdb2 = SNR_bound_instantaneous(X[:,[1]], C[:,[1]].T, b)\n",
    "print(f\"{SNRdb1:.2f} dB, {SNRdb2:.2f} dB\")\n",
    "SNRdb = SNR_bound_instantaneous(X, C.T, b)\n",
    "print(f\"{SNRdb:.2f} dB = ({SNRdb1:.2f} dB + {SNRdb2:.2f} dB)/2 = {power_to_dB((power_from_dB(SNRdb1)+power_from_dB(SNRdb2))/2):.2f} dB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88bb2e4-b593-44d7-b3f6-12de43701f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "cidx = np.lexsort((C[:,1], C[:,0]), axis=0)\n",
    "\n",
    "raster = []\n",
    "rasterSorted = []\n",
    "for k in range(nNeuron):\n",
    "    raster.append(np.nonzero(y[:,k])[0]/nT*T)\n",
    "    rasterSorted.append(np.nonzero(y[:,cidx[k]])[0]/nT*T)\n",
    "\n",
    "plt.subplots(1,2, figsize=(10, 4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.eventplot(raster, lw=0.5, color='k', label='spikes')\n",
    "plt.xlim(0, T); plt.xlabel('time'); plt.yticks([]); plt.title('raster plot'); plt.ylabel('neurons');\n",
    "plt.subplot(1,2,2)\n",
    "plt.eventplot(rasterSorted, lw=0.5, color='k', label='spikes')\n",
    "plt.xlim(0, T); plt.xlabel('time'); plt.yticks([]); plt.title('raster plot (again)'); plt.ylabel('sorted neurons');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ab1d7f",
   "metadata": {},
   "source": [
    "## What's next?\n",
    "\n",
    "Now we understand better the generative process of the model. But what we are interested is the opposite direction, that is, how do we infer the model parameters given just the observations (neural data)? This is the statistical inference problem of interest."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "xfads",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
