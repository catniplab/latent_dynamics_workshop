{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "629a16c1-3b4d-4429-b108-ead0f56d4212",
   "metadata": {},
   "source": [
    "# Inferring Latent Neural States\n",
    "\n",
    "Let's analyze some neural data using popular dimensionality reduction methods.\n",
    "We will use the folloiwng methods with progressively better modeling assumptions.\n",
    "- PCA (Principal Components Analysis)\n",
    "  - Gaussian observation\n",
    "  - Independent identical gaussian noise per neuron\n",
    "- GPFA (Gaussian Process Factor Analysis)\n",
    "  - Gaussian observation\n",
    "  - Unequal magnitude of noise per neuron\n",
    "  - Smoothness assumption on the latent trajectory\n",
    "- vLGP (varational latent Gaussian Process)\n",
    "  - Poisson observation\n",
    "  - Unequal magnitude of noise per neuron\n",
    "  - Smoothness assumption on the latent trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294ec105-85e4-46d6-858c-1825207c4dd0",
   "metadata": {},
   "source": [
    "## Load Monkey delayed-reaching task data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9865f6-11de-41cd-8af7-5d0e70060b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange\n",
    "import scipy.ndimage\n",
    "\n",
    "baseDir = 'mc_maze/data/'\n",
    "trial_info_save_path = baseDir + 'info_per_trial_{}.pkl'\n",
    "\n",
    "with (open(trial_info_save_path.format(\"train\"), \"rb\")) as openfile:\n",
    "    trial_info_train = pickle.load(openfile)\n",
    "    \n",
    "with (open(trial_info_save_path.format(\"val\"), \"rb\")) as openfile:\n",
    "    trial_info_val = pickle.load(openfile)\n",
    "    \n",
    "m5 = h5py.File(baseDir + 'monkey.hdf5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069854f9-d8b2-41c6-9c17-6b1a0008f568",
   "metadata": {},
   "outputs": [],
   "source": [
    "nTrial = m5['pos-train'].shape[0]\n",
    "nT = m5['pos-train'].shape[1]\n",
    "nNeuron = m5['spk-train'].shape[2]\n",
    "dt = 0.005  # 5 ms bin\n",
    "T = dt * nT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee16907-bb05-4d2d-bc19-654939fc0c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = m5['spk-train'][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ee2558-2156-4002-a665-102e0f350acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "kTrial = 100\n",
    "raster = []\n",
    "for kNeuron in range(nNeuron):\n",
    "    raster.append(np.nonzero(y[kTrial,:,kNeuron])[0]/nT*T)\n",
    "plt.eventplot(raster, lw=0.5, color='k', label='spikes')\n",
    "plt.xlim(0, T); plt.xlabel('time'); plt.title('raster plot'); plt.ylabel('neurons');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf1d731-a06c-458e-bf88-dc5c65106882",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100,120):\n",
    "    plt.plot(m5['pos-train'][i,:,0], m5['pos-train'][i,:,1])\n",
    "    \n",
    "plt.xlabel('X hand position'); plt.ylabel('Y hand position'); plt.grid(); plt.title('center out reaching trajectory')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d41273-1524-4e08-9e47-b9e57e1fae1b",
   "metadata": {},
   "source": [
    "## PCA\n",
    "\n",
    "In order to perform PCA, we first concatenate the the trials such that the data is of the form (trial x time) x neurons. We then smooth the data with a gaussian kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca3744c-e0d6-44e7-a835-6727d947149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smoothing data with a gaussian kernel\n",
    "data_stacked = rearrange(y, 'trial time neurons -> (trial neurons) time')\n",
    "data_smooth = scipy.ndimage.gaussian_filter1d(input = data_stacked, sigma=0.050/dt, axis=1)\n",
    "data_smooth = rearrange(data_smooth, '(trial neurons) time -> (trial time) neurons', trial=nTrial, neurons=nNeuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1c1ae4-f0c8-4920-b51b-3b94f6981126",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_centered = data_smooth - np.mean(data_smooth, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd2f3a8-3e25-4eba-b97e-46c40fdf3e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "tidx = slice(nT,2*nT)\n",
    "fig, ax = plt.subplots(1, 1, figsize =(10, 5))\n",
    "tr = np.arange(0, T, dt)\n",
    "ax.plot(tr, data_centered[tidx, 0:10]);\n",
    "ax.set_xlabel(\"time\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8234b0a-2566-4d2b-bbd7-fc55ec03a4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA using SVD\n",
    "u, s, vh = np.linalg.svd(data_centered, full_matrices=False)\n",
    "u.shape, s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83135f9c-8456-4a54-9b5d-e13a571b8d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_sv = s**2/np.sum(s**2)\n",
    "top2sv = np.sum(norm_sv[:2])\n",
    "print(\"Total observations explained by the first two principal components: {0:.2f}%\".format(top2sv*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c39cb1-bbd1-4c01-8e46-7e03d83c9503",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(15, 3))\n",
    "\n",
    "axs[0].plot(norm_sv * 100, 'o-')\n",
    "axs[1].plot(norm_sv.cumsum() * 100, 'o-')\n",
    "axs[1].set_ylim([0, 100])\n",
    "axs[2].plot(20*np.log10(norm_sv), 'o-')\n",
    "\n",
    "[(axs[k].grid(), axs[k].set_title(f''), axs[k].set_xlabel(\"PC (ordered)\")) for k in range(3)]\n",
    "axs[0].set_ylabel(\"Variance explained per PC ($\\%$)\"); \n",
    "axs[1].set_ylabel(\"Cumulative variance explained ($\\%$)\");\n",
    "axs[2].set_ylabel(\"Variance explained (dB)\"); \n",
    "fig.suptitle(\"What's the dimensionality? Inspecting variance explained by each PC defined dim\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035cf875-1ce2-412b-bc35-51392022dca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing top two PCs\n",
    "top2u = u[:, :2]\n",
    "X_hat_PCA = rearrange(top2u, '(trial time) pcs -> trial time pcs', trial=nTrial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e720c871-6377-4141-817e-a8bddfab93c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(10):\n",
    "    plt.plot(X_hat_PCA[k,:,0],  X_hat_PCA[k,:,1])\n",
    "    plt.plot(X_hat_PCA[k,-1,0], X_hat_PCA[k,-1,1], 'o')\n",
    "    \n",
    "plt.xlabel('PC1'); plt.ylabel('PC2'); plt.grid(); plt.title('2D slice')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b38478b-5db5-4e5a-9790-7ab8f60faa94",
   "metadata": {},
   "source": [
    "## GPFA\n",
    "\n",
    "We are using the implementation included in the Elephant package:\n",
    "https://elephant.readthedocs.io/en/latest/reference/gpfa.html\n",
    "\n",
    " - Yu, B. M., Cunningham, J. P., Santhanam, G., Ryu, S. I., Shenoy, K. V., & Sahani, M. (2009). Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity. Journal of Neurophysiology, 102(1), 614–635."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc9d640-1c4a-44f2-99e8-0e15bee6ddd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elephant.gpfa import GPFA\n",
    "import neo\n",
    "import quantities as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b8430f-9c18-42ea-8985-55559ac9b577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Convert to neo.SpikeTrains ---- #\n",
    "def array_to_spiketrains(array, bin_size):\n",
    "    \"\"\"Convert B x T x N spiking array to list of list of SpikeTrains\"\"\"\n",
    "    stList = []\n",
    "\n",
    "    for trial in range(array.shape[0]):\n",
    "        trialList = []\n",
    "        for channel in range(array.shape[2]):\n",
    "            times = np.nonzero(array[trial, :, channel])[0]\n",
    "            counts = array[trial, times, channel].astype(int)\n",
    "            times = np.repeat(times, counts)\n",
    "            st = neo.SpikeTrain(times*bin_size, t_stop=array.shape[1]*bin_size)\n",
    "            trialList.append(st)\n",
    "        stList.append(trialList)\n",
    "    return stList\n",
    "\n",
    "Y_st_train = array_to_spiketrains(y, dt*pq.s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b2a7c3-7005-461c-b574-a26909505dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Run GPFA ---- #\n",
    "nLatents = 3\n",
    "gpfa = GPFA(bin_size=(dt * pq.s), x_dim=nLatents)\n",
    "gpfa_val_result = gpfa.fit_transform(Y_st_train)\n",
    "\n",
    "length_scales = gpfa.params_estimated['gamma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f882d1-4bb2-4b3a-9d38-158c0186f8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hat_GPFA = rearrange(np.stack(gpfa_val_result, 0), 'trials lat time -> trials time lat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378f94ff-92cb-4101-99cf-09e8653f4781",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(10):\n",
    "    plt.plot(X_hat_GPFA[k,:,1], X_hat_GPFA[k,:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc83ff87-7a68-4ba8-b5c2-7a14962db01b",
   "metadata": {},
   "source": [
    "## vLGP\n",
    "\n",
    " - Zhao, Y., & Park, I. M. (2017). Variational Latent Gaussian Process for Recovering Single-Trial Dynamics from Population Spike Trains. Neural Computation, 29(5), 1293–1316. arXiv.\n",
    " - Nam, H. (2015). Poisson Extension of Gaussian Process Factor Analysis for Modelling Spiking Neural Populations (J. Macke (ed.)). Eberhard-Karls-Universität Tübingen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536afea6-c90a-42de-861f-eff8fb37d885",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vlgpax.kernel import RBF\n",
    "from vlgpax import Session, vi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e352d8e-36c2-4f44-b224-21ab19fcb761",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session(dt)\n",
    "\n",
    "# Session is the top level container of data. Two arguments, binsize and unit of time, are required at construction.\n",
    "for i, yy in enumerate(y):\n",
    "    session.add_trial(i + 1, y = yy)  # Add trials to the session.\n",
    "\n",
    "# Build the model\n",
    "kernel = RBF(scale = 1., lengthscale = 25 * dt)  # RBF kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13a998a-1f0c-4aed-9d41-e2a16a0e769b",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 20221011\n",
    "np.random.seed(random_seed)\n",
    "session, params = vi.fit(session, n_factors=nLatents, kernel=kernel, seed=random_seed, max_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0564d5b4-b277-4724-b409-293e507fd4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hat_VLGP = rearrange(session.z, '(trials time) lat -> trials time lat', time=nT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac9bca0-3010-491d-8c06-dde289aa8422",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(1,3,figsize=(12,4))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "for k in range(10):\n",
    "    plt.plot(X_hat_PCA[k,:,0],  X_hat_PCA[k,:,1])\n",
    "    plt.plot(X_hat_PCA[k,-1,0], X_hat_PCA[k,-1,1], 'o')\n",
    "plt.xticks([]); plt.yticks([]); plt.gca().axis('equal')\n",
    "plt.title('PCA')\n",
    "    \n",
    "plt.subplot(1,3,2)\n",
    "for k in range(10):\n",
    "    plt.plot(X_hat_GPFA[k,:,0],  X_hat_GPFA[k,:,1])\n",
    "    plt.plot(X_hat_GPFA[k,-1,0], X_hat_GPFA[k,-1,1], 'o')\n",
    "plt.xticks([]); plt.yticks([]); plt.gca().axis('equal')\n",
    "plt.title(\"GPFA\");\n",
    "        \n",
    "plt.subplot(1,3,3)\n",
    "for k in range(10):\n",
    "    plt.plot(X_hat_VLGP[k,:,0],  X_hat_VLGP[k,:,1])\n",
    "    plt.plot(X_hat_VLGP[k,-1,0], X_hat_VLGP[k,-1,1], 'o')\n",
    "plt.xticks([]); plt.yticks([]); plt.gca().axis('equal')\n",
    "plt.title(\"vLGP\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7227fbe4-5724-4797-a2e2-57abef578bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "m5.close() # closing the data file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
